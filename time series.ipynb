{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPFlfLLAsjbCQBd1tXcHZnC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install optuna imblearn\n","\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import TimeSeriesSplit\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import precision_recall_curve, auc, roc_auc_score, f1_score\n","import xgboost as xgb\n","import optuna\n","from imblearn.combine import SMOTETomek\n","from joblib import Parallel, delayed\n","\n","def generate_time_series_dataset(n_samples, n_features, imbalance_ratio):\n","\n","    Args:\n","    n_samples: total rows (>= 10000)\n","    n_features: number of numeric features\n","    imbalance_ratio: majority:minority ratio (e.g., 10 -> 10:1)\n","    Returns:\n","    pd.DataFrame with columns: ['timestamp', 'f0',...,'f{n-1}', 'target']\n","    # timestamps at regular intervals\n","    timestamps = pd.date_range(start='2020-01-01', periods=n_samples, freq='h') # Changed 'H' to 'h'\n","\n","    # latent signal that generates rare events\n","    t = np.arange(n_samples)\n","    seasonal = 0.5 * np.sin(2 * np.pi * t / 24) # daily seasonality\n","    trend = 0.0005 * t\n","\n","    features = []\n","    for i in range(n_features):\n","        noise = np.random.normal(scale=1.0, size=n_samples)\n","        decay = np.exp(- (i+1)/20.0)\n","        feat = decay * (seasonal + trend) + 0.1 * np.random.normal(size=n_samples) + 0.3 * np.sin(2*np.pi*t/(24*(i+1))) + noise*0.2\n","        features.append(feat)\n","    X = np.vstack(features).T\n","\n","    # Create a score that correlates with rare events\n","    score = (X[:, :3].sum(axis=1) + 0.5 * np.random.randn(n_samples) + 0.5 * seasonal + 0.1*t)\n","\n","    # Convert score to probabilities via sigmoid\n","    def sigmoid(x):\n","        return 1 / (1 + np.exp(-x/3.0))\n","\n","    base_prob = sigmoid(score)\n","\n","    # We need an overall minority rate ~ 1/(imbalance_ratio+1)\n","    desired_minority_rate = 1.0 / (imbalance_ratio + 1.0)\n","\n","    # scale base_prob so mean equals desired rate\n","    scaled_prob = base_prob * (desired_minority_rate / base_prob.mean())\n","    scaled_prob = np.clip(scaled_prob, 0, 1)\n","\n","    y = np.random.binomial(1, scaled_prob)\n","\n","    df = pd.DataFrame(X, columns=[f'f{i}' for i in range(n_features)])\n","    df['timestamp'] = timestamps\n","    df['target'] = y\n","\n","    # shuffle not allowed for time-series; keep chronological order\n","    return df\n","\n","\n","# preprocessing.py\n","def create_features(df):\n","    # Example feature engineering: lag features, rolling stats\n","    df = df.copy()\n","    df = df.sort_values('timestamp')\n","\n","    # create a few lags for first 3 features\n","    for f in ['f0', 'f1', 'f2']:\n","        df[f + '_lag1'] = df[f].shift(1)\n","        df[f + '_lag24'] = df[f].shift(24)\n","        df[f + '_rmean24'] = df[f].rolling(window=24, min_periods=1).mean()\n","    return df\n","\n","# baseline_lr.py\n","RANDOM_SEED = 42\n","\n","# generate data\n","df = generate_time_series_dataset(n_samples=20000, n_features=15, imbalance_ratio=9.0)\n","\n","# fe\n","df = create_features(df)\n","# Drop rows with NaN values created by lag features\n","df.dropna(inplace=True)\n","\n","features = [c for c in df.columns if c not in ['timestamp', 'target']]\n","X = df[features].values\n","y = df['target'].values\n","\n","# scale\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# TimeSeriesSplit\n","tscv = TimeSeriesSplit(n_splits=5)\n","\n","lr_metrics = []\n","for train_idx, test_idx in tscv.split(X_scaled):\n","    X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n","    y_train, y_test = y[train_idx], y[test_idx]\n","\n","    model = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=RANDOM_SEED)\n","    model.fit(X_train, y_train)\n","    y_proba = model.predict_proba(X_test)[:, 1]\n","    y_pred = model.predict(X_test)\n","\n","    precision, recall, _ = precision_recall_curve(y_test, y_proba)\n","    auc_pr = auc(recall, precision)\n","    auc_roc = roc_auc_score(y_test, y_proba)\n","    f1 = f1_score(y_test, y_pred)\n","\n","    lr_metrics.append({'auc_pr': auc_pr, 'auc_roc': auc_roc, 'f1': f1})\n","\n","print('Logistic Regression (TimeSeriesSplit) metrics (mean +/- std):')\n","for k in ['auc_pr', 'auc_roc', 'f1']:\n","    vals = [m[k] for m in lr_metrics]\n","    print(k, np.mean(vals), np.std(vals))\n","\n","# xgb_unbalanced.py\n","\n","# reuse df, features, X_scaled, y from earlier generation and preprocessing\n","def evaluate_xgb_unbalanced(X, y, tscv):\n","    metrics = []\n","    for train_idx, test_idx in tscv.split(X):\n","        X_train, X_test = X[train_idx], X[test_idx]\n","        y_train, y_test = y[train_idx], y[test_idx]\n","\n","\n","        dtrain = xgb.DMatrix(X_train, label=y_train)\n","        dtest = xgb.DMatrix(X_test, label=y_test)\n","\n","\n","        params = {\n","        'objective': 'binary:logistic',\n","        'eval_metric': 'logloss',\n","        'seed': 42,\n","        'verbosity': 0\n","        }\n","        bst = xgb.train(params, dtrain, num_boost_round=200)\n","        y_proba = bst.predict(dtest)\n","        y_pred = (y_proba > 0.5).astype(int)\n","\n","\n","        precision, recall, _ = precision_recall_curve(y_test, y_proba)\n","        auc_pr = auc(recall, precision)\n","        auc_roc = roc_auc_score(y_test, y_proba)\n","        f1 = f1_score(y_test, y_pred)\n","        metrics.append({'auc_pr': auc_pr, 'auc_roc': auc_roc, 'f1': f1})\n","    return metrics\n","\n","\n","metrics_unbal = evaluate_xgb_unbalanced(X_scaled, y, tscv)\n","print('Unbalanced XGBoost metrics (mean +/- std):')\n","for k in ['auc_pr', 'auc_roc', 'f1']:\n","    vals = [m[k] for m in metrics_unbal]\n","    print(k, np.mean(vals), np.std(vals))\n","\n","# xgb_optuna.py\n","\n","# We'll optimize hyperparameters using a custom objective that performs TimeSeriesSplit\n","\n","def objective(trial, X, y, tscv):\n","    param = {\n","    'verbosity': 0,\n","    'objective': 'binary:logistic',\n","    'booster': 'gbtree',\n","    'tree_method': 'hist',\n","    'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n","    'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n","    'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.3, 1.0),\n","    'subsample': trial.suggest_uniform('subsample', 0.4, 1.0),\n","    'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.3),\n","    'max_depth': trial.suggest_int('max_depth', 3, 10),\n","    'min_child_weight': trial.suggest_int('min_child_weight', 1, 20),\n","    'gamma': trial.suggest_uniform('gamma', 0.0, 5.0),\n","    # We'll not set scale_pos_weight here; instead use resampling for some trials\n","    }\n","\n","\n","    pr_scores = []\n","\n","for train_idx, test_idx in tscv.split(X):\n","        X_train, X_test = X[train_idx], X[test_idx]\n","        y_train, y_test = y[train_idx], y[test_idx]\n","\n","\n","  # Apply SMOTE-Tomek on training fold only\n","        smt = SMOTETomek(random_state=42)\n","        X_res, y_res = smt.fit_resample(X_train, y_train)\n","\n","\n","        dtrain = xgb.DMatrix(X_res, label=y_res)\n","        dtest = xgb.DMatrix(X_test, label=y_test)\n","\n","\n","  # early stopping\n","        evallist = [(dtest, 'eval')]\n","        booster = xgb.train(param, dtrain, num_boost_round=1000, evals=evallist,\n","        early_stopping_rounds=30, verbose_eval=False)\n","        y_proba = booster.predict(dtest, ntree_limit=booster.best_ntree_limit)\n","\n","\n","        precision, recall, _ = precision_recall_curve(y_test, y_proba)\n","        auc_pr = auc(recall, precision)\n","        pr_scores.append(auc_pr)\n","# We return mean PR AUC across folds\n","    return float(np.mean(pr_scores))\n",""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0FFW64rzHgQs","executionInfo":{"status":"ok","timestamp":1763623302417,"user_tz":-330,"elapsed":15133,"user":{"displayName":"Shanmugapriya B","userId":"12414382680676837432"}},"outputId":"79d4236d-b636-47d7-bd8e-4d2a552ce382"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting optuna\n","  Downloading optuna-4.6.0-py3-none-any.whl.metadata (17 kB)\n","Collecting imblearn\n","  Downloading imblearn-0.0-py2.py3-none-any.whl.metadata (355 bytes)\n","Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.17.1)\n","Collecting colorlog (from optuna)\n","  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n","Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.44)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.3)\n","Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.12/dist-packages (from imblearn) (0.14.0)\n","Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n","Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n","Requirement already satisfied: scipy<2,>=1.11.4 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn->imblearn) (1.16.3)\n","Requirement already satisfied: scikit-learn<2,>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn->imblearn) (1.6.1)\n","Requirement already satisfied: joblib<2,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn->imblearn) (1.5.2)\n","Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn->imblearn) (3.6.0)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n","Downloading optuna-4.6.0-py3-none-any.whl (404 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.7/404.7 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n","Downloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n","Installing collected packages: colorlog, optuna, imblearn\n","Successfully installed colorlog-6.10.1 imblearn-0.0 optuna-4.6.0\n","Logistic Regression (TimeSeriesSplit) metrics (mean +/- std):\n","auc_pr 0.0969682187999805 0.011675083161486315\n","auc_roc 0.48087007765473055 0.01483468335271087\n","f1 0.12093821937865265 0.03865438655913782\n","Unbalanced XGBoost metrics (mean +/- std):\n","auc_pr 0.09933232575032924 0.009055501505288858\n","auc_roc 0.488593099426167 0.009952158009421194\n","f1 0.018508380062697043 0.02609683983592565\n"]}]}]}